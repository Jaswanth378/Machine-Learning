!apt-get update
!apt-get install -y ffmpeg

import os
os.environ['SDL_VIDEODRIVER'] = 'dummy'
os.environ['DISPLAY'] = ':0'

import gym
import numpy as np
from collections import deque
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import random
from gym.wrappers import RecordVideo


GAMMA = 0.99
BUFFER_SIZE = 10000
BATCH_SIZE = 128
LEARNING_RATE = 0.001
UPDATE_EVERY = 10
EXPLORATION_RATE_START = 1.0
EXPLORATION_RATE_END = 0.01
EXPLORATION_DECAY_RATE = 0.995
EPSILON_GREEDY_STEPS = 1000000

# Starting the environment
env = gym.make("LunarLander-v2")
state_size = env.observation_space.shape[0]
action_size = env.action_space.n


replay_buffer = deque(maxlen=BUFFER_SIZE)

# Initializing the Q-Network
inputs = layers.Input(shape=(state_size,))
x = layers.Dense(128, activation="relu")(inputs)
x = layers.Dense(128, activation="relu")(x)
outputs = layers.Dense(action_size, activation="linear")(x)
model = keras.Model(inputs=inputs, outputs=outputs)
model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
              loss=keras.losses.Huber())

# Initializing the target network
target_model = keras.models.clone_model(model)
target_model.set_weights(model.get_weights())

# Initializing the variables
steps = 0
exploration_rate = EXPLORATION_RATE_START
episode_rewards = []
episode_losses = []
successful_landings = 0

# Training the loop for 1000 episodes
num_episodes = 1000
for episode in range(num_episodes):
    state = env.reset()
    episode_reward = 0
    episode_loss = 0
    done = False

    while not done:
        
        if np.random.rand() < exploration_rate:
            action = env.action_space.sample()  
        else:
            q_values = model(np.expand_dims(state, axis=0))
            action = np.argmax(q_values[0])  

        
        next_state, reward, done, _ = env.step(action)
        episode_reward += reward

       
        replay_buffer.append((state, action, reward, next_state, done))

        
        if len(replay_buffer) >= BATCH_SIZE and steps % UPDATE_EVERY == 0:
            sample_batch = random.sample(replay_buffer, BATCH_SIZE)

            
            states = np.array([transition[0] for transition in sample_batch])
            actions = np.array([transition[1] for transition in sample_batch])
            rewards = np.array([transition[2] for transition in sample_batch])
            next_states = np.array([transition[3] for transition in sample_batch])
            dones = np.array([transition[4] for transition in sample_batch])

            
            next_q_values = target_model.predict(next_states)
            target_values = rewards + GAMMA * np.max(next_q_values, axis=1) * (1 - dones)

            # Updating the Q-Network
            with tf.GradientTape() as tape:
                q_values = model(states)
                one_hot_actions = tf.one_hot(actions, action_size)
                masked_q_values = tf.reduce_sum(one_hot_actions * q_values, axis=1)
                loss = tf.reduce_mean(keras.losses.Huber()(target_values, masked_q_values))

            episode_loss += loss
            grads = tape.gradient(loss, model.trainable_variables)
            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))

        # Updating the target network
        if steps % (UPDATE_EVERY * 4) == 0:
            target_model.set_weights(model.get_weights())

        # Updating the exploration rate
        exploration_rate = max(EXPLORATION_RATE_END, exploration_rate * EXPLORATION_DECAY_RATE)

        state = next_state
        steps += 1

    episode_rewards.append(episode_reward)
    episode_losses.append(episode_loss / steps)

    if episode_reward > 200:  
        successful_landings += 1

    print(f"Episode: {episode}, Reward: {episode_reward}")

# Plotting the results
plt.figure(figsize=(8, 5))
plt.plot(episode_rewards)
plt.title("Episode Rewards")
plt.xlabel("Episode")
plt.ylabel("Reward")
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 5))
plt.plot(episode_losses)
plt.title("Episode Losses")
plt.xlabel("Episode")
plt.ylabel("Loss")
plt.tight_layout()
plt.show()

# Saving  the trained model
model.save("lunar_lander_model.keras")

